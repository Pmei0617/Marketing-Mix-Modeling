---
title: "Marketing Mix Modeling (WSB)"
author: 'Project Member: Daniel Reinhardt, ShengYa Mei (Peter)'
date: "2023-04-15"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Project Overview

This project was developed using data supplied by MMA, a consulting firm that specializes in marketing mix models. The purpose of this project is to understand how sales for Brand C relate to factors in the marketplace (own marketing efforts, others’ marketing efforts, environmental factors). This information could then be used to (i) assess the relative impact of different elements of the marketing mix and (ii) volume forecasting.

This is an integrated dataset with 179 weeks (Feb 2000 – Jul 2003) of observations for a variety of marketing mix variables. Brand C is one of the big players in a fairly commoditized product category. Brands R, E, P and U are some of the other brands in the category. Brand C and U are owned by the same company. Some of the measures in the dataset should look familiar, while others may be new. The key dependent variable in the dataset is equivalent units sales volume. You have selling price information for brands C, E, and P. The variable disacv_c not only measures how deep of a discount was offered for brand C, but also how prevalent that discount was across sales outlets.

A unique measure of promotional activity included in the dataset is expressed in terms of coupon valuation within an FSI drop, and how big the coupon drop was (in terms of circulation). This measure is reported as two variables for brand C contingent on holiday or non-holiday time periods. The dataset also has information about coupon drops for competitors R and E.

Information from Nielsen Media Research is incorporated into the dataset as TV GRP information for commercials featuring brands C and U. A gross rating point (GRP) is a variable used to measure the “impact” of television advertising. There is also an indicator variable for the thematic focus of the television advertising message.

The dataset also includes information about the prevalence of brand C’s bonus pack offering, a measure of line length per store expressed as rolling average of SKU’s per store, and (using panel data) percent share of brand C that is sold through Wal*Mart.

<br>

### Definitions of variables in dataset:

**week**: Week of observations <br>
**weeknumber**: Week number <br>
**month**: Month <br>
**year**: Year <br>
**eq_volum_c**: Equivalent unit sales volume for brand C (the dependent variable) <br>
**disacv_c**: Brand C %ACV * % Discount (This variable captures depth of price discount and how prevalent it was. That is, weighted average price discount) <br>
**bonusacv**: %ACV for stores in which brand C bonus pack had sales <br>
**price_c**: Brand C price per equivalent unit (non promoted price) <br>
**price_e**: Brand E price per equivalent unit (non promoted price) <br>
**price_p**: Private label price per equivalent unit (non promoted price) <br>
**tvgrp_c**: Brand C TV GRPs (GRPs are reach TIMES frequency or the number of people viewing the commercial and how many times they see it.) <br>
**tvgrp_u**: Brand U TV GRPs (GRPs are reach TIMES frequency or the number of people viewing the commercial and how many times they see it.) <br>
**trustad**: Theme of Brand C TV advertising focused on the message “Trusted”. Included to indicate times when this ad ran. <br>
**fsi_holi**: Brand C Holiday FSIs (coupon value * circulation) <br>
**fsi_non**: Brand C Non-Holiday FSIs (coupon value * circulation) <br>
**fsi_comp**: Brand E or R FSIs (coupon value * circulation) <br>
**itemstor**: Number of Brand C items sold per store – rolling 13 week average <br>
**walmart**: Wal*Mart share

<br>

## Part 1: Data exploration. Discover key findings

#### Install required packages and load
```{r, results='hide', message=FALSE, warning=FALSE}
# Uncomment to install packages for the first time

#install.packages("tidyverse")
#install.packages("vtable")
#install.packages("gmodels")
#install.packages("GGally")
#install.packages("performance")
#install.packages("car")
#install.packages("reshape2")
#install.packages("psych")


library(tidyverse)
library(vtable)
library(gmodels)
library(GGally) 
library(performance)
library(car)
library(reshape2)
library(corrplot)
library(psych) 
library(lmtest)
library(gridExtra)

```

```{r}
# Set working directory for the location of regression dataset
Datset = read.csv("MMM regression dataset.csv",header=TRUE)
attach(Datset) 
```

#### Summary statistics
```{r}
# Generate basic summary statistics
sumtable(Datset)
summary(Datset)


# generate summary statistic for our response variable
summary(eq_volum)
```

Based on the summary statistics, we see that brand E has the highest average price per equivalent unit whereas brand P has the lowest. For our response variable, we observe a huge difference between the mean and the maximum value. Eq_volum averages around 4.4M with minimum of 2.8M and maximum of 15.5M. This explains there are extreme outliers towards the upper bound for the eq_volum variable. For coupon variables depicted by FSI, we can see that our competitors (E or R) have higher average promotional activity compared to brand C in terms of dropping coupons.

Histogram and boxplot for eq_volum (response variable) shows right skewed distribution. There are a few extremes at the larger end.

```{r}
ggplot(Datset, aes(x=eq_volum)) + geom_histogram(bins = 50)
ggplot(Datset, aes(x=eq_volum)) + geom_boxplot()
```

#### Dealing with extreme outliers
Histogram and boxplot for eq_volum (response variable) shows right skewed distribution. There are a few extremes at the larger end. To mitigate the effect of extreme outliers, we will perform a log transformation on our response variable.

```{r}
# Create a new column to store log transformed sale volumn
Datset$log_eq_volum <- log(Datset$eq_volum)
# Visualize distribution again
ggplot(Datset, aes(x=log_eq_volum)) + geom_histogram(bins = 50)
```

Slightly better but not still not normally distributed. Because we have very limited observations (179), we will consider removing our outliers for now.

We will create a new dataset without outliers to see if it does make a better model in the end.

```{r}
# Create dataset with outliers removed

summary(eq_volum)

dim(Datset[eq_volum < 8000000,]) # Taking out outliers with eq_volum >= 8000000 left us with 169 observations which is 10 less than original data.
Datset[eq_volum >= 8000000,] # We see that the outliers all fall during the last few weeks in a year or on the first week of year
```


```{r}
outlier_removed <- Datset[eq_volum < 8000000,]
ggplot(outlier_removed, aes(x=log(eq_volum))) + geom_histogram(bins = 50)
```

Closer to normal distribution with outliers removed.

#### Looking at price variations
Lets now look at how price varies by week and by year. We will use line chart to visualize how Brand C prices varies across weeks in each year compared to Brand E and P. Because we have very limited observations (179), we will not consider removing our outliers moving forward.

We will attach the dataset again after adding in the new log transformed response variable.
```{r}
attach(Datset)

dim(Datset)

# Mean price for Brand C, E, P grouped by week and by year
C_price_week_year <- aggregate(price_c, list(weeknumber, year), mean)
E_price_week_year <- aggregate(price_e, list(weeknumber, year), mean)
P_price_week_year <- aggregate(price_p, list(weeknumber, year), mean)

# Rename the column headers for the three brands
C_price_week_year <- C_price_week_year %>% 
  rename(
    C_price = x,
    week_number = Group.1,
    year = Group.2
  )

E_price_week_year <- E_price_week_year %>% 
  rename(
    E_price = x,
    week_number = Group.1,
    year = Group.2
  )

P_price_week_year <- P_price_week_year %>% 
  rename(
    P_price = x,
    week_number = Group.1,
    year = Group.2
  )

# Generate a line plot to visualize Brand C, E and P prices across week by year
lineplot_C_price_week_year <- C_price_week_year %>% 
  arrange(year, week_number) %>%
  ggplot() +
  geom_line(aes(week_number, C_price, color = factor(year), group = year)) +
  scale_color_brewer(palette = "Set1") +  # Choose a color palette for the year variable
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))

lineplot_E_price_week_year <- E_price_week_year %>% 
  arrange(year, week_number) %>%
  ggplot() +
  geom_line(aes(week_number, E_price, color = factor(year), group = year)) +
  scale_color_brewer(palette = "Set1") +  # Choose a color palette for the year variable
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))

lineplot_P_price_week_year <- P_price_week_year %>% 
  arrange(year, week_number) %>%
  ggplot() +
  geom_line(aes(week_number, P_price, color = factor(year), group = year)) +
  scale_color_brewer(palette = "Set1") +  # Choose a color palette for the year variable
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size = 5))


# Compare price across week by year among Brands C, E and P
grid.arrange(lineplot_C_price_week_year, lineplot_E_price_week_year, lineplot_P_price_week_year, nrow = 2)

```


Looking at price variation across Brand C, E and P, we do not observe any similarity in pricing strategy. We see that prices for Brand C remain relatively flat in some periods compared to that of Brand E and P. Brand C’s pricing strategy is much more stable. In 2003, Brand C’s pricing dropped significantly around week 20 whereas Brand E had a relatively more stable pricing strategy compared to that of Brand C and P. The cause of the drop in Brand C can be examined with more business context.

## Part 2: Exploring bivariate relationships using correlation

We will use x-y plot in showing how sales vary by price

```{r}
corr_C <- ggplot(Datset, aes(x=price_c, y=eq_volum)) + 
  geom_point()+
  geom_smooth(method=lm)
corr_E <- ggplot(Datset, aes(x=price_e, y=eq_volum)) + 
  geom_point()+
  geom_smooth(method=lm)
corr_P <- ggplot(Datset, aes(x=price_p, y=eq_volum)) + 
  geom_point()+
  geom_smooth(method=lm)

grid.arrange(corr_C, corr_E, corr_P, nrow = 2)
```

No strong correlation observed between eq_volum and each brand's pricing.
The price of Brand C has a positive correlation, which is odd since you would assume that a higher price would lead to reduced sales. This indicates that Brand C is relatively price inelastic.

We will generate a correlation matrix to see association between each variables.    
```{r}
data.new<-Datset %>% select (eq_volum,price_c,price_e,price_p,tvgrp_c,tvgrp_u,trustad,fsi_holi,fsi_non,fsi_comp,disacv_c,bonusacv,itemstor,walmart)

corr_matrix <- cor(data.new)
corrplot(corr_matrix, method = "color", type = "upper", tl.col = "black",
         tl.srt = 45, addCoef.col = "black", col = colorRampPalette(c("red", "white", "darkgreen"))(100), tl.cex = 0.8, number.cex = 0.7)
```


According to our x-y plots, changes in pricing of Brand E seems to have very little effect on Brand C sales, but Brand C sales seem to increase somewhat when the price of Brand P increases. However, as shown by the correlation matrix, the correlation for each of these associations is fairly weak. Additionally, as shown below, the p-value is fairly high for the correlation between price_e and eq_volum (p=0.55), but is a bit lower for the relationships between eq_volum/price_c (p=0.093) and eq_volum/price_p (p=0.035).


Check for significance of correlations that show descent association between two variables.
```{r}
# Check for significance of correlations
cor(price_c,eq_volum)
cor.test(price_c,eq_volum)$p.value #Not Significant

cor(price_p,eq_volum)
cor.test(price_p,eq_volum)$p.value #Significant

cor(price_e,eq_volum)
cor.test(price_e,eq_volum)$p.value #Not Significant

cor(disacv_c,eq_volum)
cor.test(disacv_c,eq_volum)$p.value #Significant

cor(bonusacv,eq_volum)
cor.test(bonusacv,eq_volum)$p.value #Significant

cor(tvgrp_c,eq_volum)
cor.test(tvgrp_c,eq_volum)$p.value #Significant

cor(fsi_holi,eq_volum)
cor.test(fsi_holi,eq_volum)$p.value #Significant

cor(bonusacv,disacv_c)
cor.test(bonusacv,disacv_c)$p.value #Significant
```

Correlation matrix shows no strong correlation between brand C’s sales and price of all brands. Variable disacv_c shows the strongest correlation with our response variable. This is expected because price discounts have a strong positive correlation with sales in general. We also see a decent positive correlation between tvgrp_c and sales, indicating the successful translation of high impact television campaigns to increased sales.

We will now graph x-y plots of variables with descent or high association with sales.
```{r}
highcorr_disacv_c <- ggplot(Datset, aes(x=eq_volum, y=disacv_c)) + 
  geom_point()+
  geom_smooth(method=lm)

highcorr_tvgrp_c <- ggplot(Datset, aes(x=eq_volum, y=tvgrp_c)) + 
  geom_point()+
  geom_smooth(method=lm)

highcorr_bonusacv <- ggplot(Datset, aes(x=eq_volum, y=bonusacv)) + 
  geom_point()+
  geom_smooth(method=lm)

highcorr_fsi_holi <- ggplot(Datset, aes(x=eq_volum, y=fsi_holi)) + 
  geom_point()+
  geom_smooth(method=lm)

grid.arrange(highcorr_disacv_c, highcorr_tvgrp_c, highcorr_bonusacv, highcorr_fsi_holi, nrow = 2)
```

* Brand C sales is positively correlated to both disacv_c and bonusacv, indicating a fairly strong association between increased sales and Brand C’s discounting efforts.<br>
* TV advertising campaigns also seem to have a positive influence on Brand C sales, as there is a fairly strong positive correlation between tvgrp_c and eq_volum.<br>
* There is also a fairly strong association between fsi_holi and eq_volum, indicating that holiday coupons do seem to translate into increased sales.<br>

These associations will be important to keep in mind as we build and modify our regression model.



Additional plot showing correlation between independent variables that are high and significant.
```{r}
# x-y plots of independent variables with high or descent association with one another
# (Not including response variable)

ggplot(Datset, aes(x=bonusacv, y=disacv_c)) + 
  geom_point()+
  geom_smooth(method=lm)
```


## Part 3: Building regression models for marketing mix modeling

### Model 0
mod0 includes all price variables regressed onto eq_volum.
```{r}
mod0<- lm(eq_volum ~ price_c+price_p+price_e)
summary(mod0)
```

The adjusted R-Square is low, but the relationship is not linear so a log transformation may improve this

### Model 1
mod1 includes all price variables regressed onto the log transformation of eq_volum.
```{r}
mod1<- lm(log(eq_volum) ~ price_c+price_p+price_e)
summary(mod1)
```

The adjusted R-Square is better, but still quite low. 

There is a high degree of seasonality in sales, with most eq_volum occuring in the 52nd week of the year, as shown in the following.
```{r}
ggplot(Datset, aes(weeknumber)) + 
  geom_line(aes(y = eq_volum, colour = "eq_volum"),stat = "summary", fun = "mean") +
    scale_x_continuous(breaks = seq(min(Datset$weeknumber), max(Datset$weeknumber), by = 5))
```

Let's create a dummy variable called holiday to account for the spike in sales around the last week of the year.
```{r}
# Create holiday dummy
Datset$holiday <- ifelse(weeknumber == 52, 1, 0)
attach(Datset)
```


### Model 2
mod2 includes all variables regressed onto log(eq_volum), but also includes a dummy variable "holiday" to account for seasonality, as well as adding discounting activities and removing price_e which seems to have little relationship with eq_volum.
```{r}
mod2<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday)
summary(mod2)
```

The R-Square for mod2 is great (0.8422). What about their TV activities?

### Model 3
mod3 includes the tvgrp variables to see if their television activities can help predict sales.
```{r}
mod3<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u)
summary(mod3)
```

The R-Square looks really good (0.8699) and vif looks good
```{r}
vif(mod3) 
```

### Model 4 & 5
mod4 and mod5 assess the predictive power of additionally adding trustad and walmart, respectively.
```{r}
mod4<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u+trustad)
summary(mod4)
vif(mod4)
```

We see trustad not significant; no predictive power.

```{r}
mod5<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u+walmart)
summary(mod5)
vif(mod5)
```

We see walmart not significant; no predictive power.

### Model 6
mod6 assesses the predictive power of coupon drop by Brand C on holidays an non holidays and of competitors.
```{r}
mod6<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u+fsi_comp+fsi_holi+fsi_non)
summary(mod6)
vif(mod6)
```

None of these variables were significant. No predictive power of couponing.

### Model 7
mod7 assesses the predictive power of itemstor, an indicator of line length at stores.
```{r}
mod7<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u+itemstor)
summary(mod7)
vif(mod7)
```

This improves R-Square, but also bumps vif for bonusacv - not to a concerning amount though.

At this point, there could be an interaction between bonusacv and disacv_c because we felt these terms might be related. For example, when there is a bonus product included for Brand C, it may be included as part of a discount promotion. So let's add this interaction term and see how we do.

### Model 8
mod8 includes all adding interaction term between disacv and bonusacv.
```{r}
mod8<- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u+itemstor+disacv_c*bonusacv)
summary(mod8)
vif(mod8)
```

This increases the vif to an undesirable level for several variables, so we will remove this variable.

## Part 4: Identify best model to use for prediction

**Our best model is mod7**
```{r}
summary(mod7)
```

The R-Square of our final model is 0.91. All P-values were less than 0.001 except for price_p, which was 0.49. Although this is close to the 0.5 cutoff, we decided to leave it in since we felt that the price of a competing brand might be an important predictor to Brand C’s performance. This indicates that each variable has predictive value in our model.

Looking at mod7, price discounts is the most effective marketing activity for Brand C in terms of generating sales. Independent variable ‘disacv_c’ has a coefficient estimate of around 0.03. Holding all other variables constant, an one unit increase in disacv_c is associated with a exp(0.0302460+12.8073911) increase in sales which is around 376110.1 unit sales increase for
Brand C.

Bonus packs comes second in terms of effective marketing activity for Brand C with a log transformed coefficient estimate of 0.0073333. TV advertisement comes third with a log transformed coefficient estimate of 0.0005380. Couponing activities (fsi) for Brand C and their competitors were found to have no significant association with sales hence they were excluded from
the model in predicting sales.

Holiday is also an important factor in predicting sales. Holiday variable takes binary input of 1 or 0. For our model interpretation, the holiday's reference group is set at 0 with a log transformed coefficient estimate of 0.5240258. We can interpret it as, sales on holidays are on average about exp(0.5240258+12.8073911) = 616255.5 more than that on non-holidays while keeping all other variables constant.

For effects of competitive activities on Brand C’s sales, we do not find any association between price of Brand E and sales of Brand C in our earlier models hence it was not included in our final model mod7. As for Brand P’s pricing, we do observe a significant association. As price for Brand P increases, sales for Brand C increases. This is anticipated because both brands sell products in the same category so Brand C could be perceived as a purchase alternative if there is an increase in price of Brand P.

Variable ‘tvgrp_u’ shows TV commercial viewing frequency for Brand U. The fact that Brand C and U are owned by the same company suggests customers who viewed Brand U’s commercials could be introduced to Brand C in their customer journey. This helps explain why ‘tvgrp_u’ is positively associated with our sales.

### Justifying choices of independent variables in mod7

Price seemed like a logical starting point for our regression analysis because sales are typically closely related to price. We started by regressing all brand prices onto a log transformation of eq_volum. This resulted in a low R-Square value (0.03). We then decided to add in variables associated with discount (disavc_c and bonusacv) because we assumed (and informed by correlation analysis) that there would be a positive relationship between discounted items and unit sales. At this point, we also removed price_e because it had a minimal correlation with eq_volum and had a high p-value in the model, clearly making it a poor predictor of eq_volum. We also added a “dummy” variable to account for seasonality that seemed to be occurring at week 52 each year.

The effect of these three changes resulted in the R-Square jumping to 0.84 - a significant increase. We then decided to evaluate the effect of tvgrp_c and tvgrp_u, given that we saw a strong correlation between tvgrp_c and eq_volum. Adding these effects produced an R-Square of 0.87.

Itemstor as an evaluation of a brand’s presence within stores could be tied to sales because when consumers see more product lines of the same brand being offered they may be more likely to purchase from that brand. The addition of Itemstor brought the R-Square up to 0.91. At this point we decided to evaluate VIF to evaluate whether there was multicollinearity, which showed no evidence of multicollinearity as all VIF scores were below 2.3.

We then considered adding trustad and walmart but decided against including them because neither showed significance and both resulted in a reduced R-Square value. We also considered factoring in coupon variables as well (fsi_holi, fsi_non, fsi_comp), but non of these variables improved the model either. This was surprising, since one would assume that coupon drops would result in increased unit sales.

We also considered adding an interaction term between bonusacv and disacv_c because we felt that these two variables might be related. Although the addition of this interaction term resulted in an increased R-Square (0.92), it also resulted in a VIF of 6 for disacv_c, 10 for bonusacv, and 15 for the interaction factor. For this reason, we felt it better to remove this interaction factor from the model.

The final model was able to account for seasonality through the use of the “holiday” dummy variable for week 52. It also had fairly low VIFs among all independent variables with only two variables exhibiting a score higher than 2 (bonusacv=5.75, itemstor=4.38).


Now we will look at model fit of actual vs. predicted sales.

```{r}
logpredicted7 <- predict(mod7)
predicted7 <- exp(logpredicted7)
ggplot(Datset, aes(x=eq_volum, y=predicted7)) + 
  coord_cartesian(xlim =c(0, 15000000), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)

ggplot(Datset, aes(observation)) + 
  geom_line(aes(y = eq_volum, colour = "eq_volum"),stat = "summary", fun = "mean") + 
  geom_line(aes(y = predicted7, colour = "predicted7"),stat = "summary", fun = "mean")
```

Our model did a very good job in predicting equivalent sales volume over time based on the key predictors that we identified. It did struggle slightly to accurately predict the annual sales spikes, over predicting the first and under predicting the second and third. Some of the struggles may stem from the fact that sales spikes actually lasted for two weeks while this model was only trained on one week.

### Assumption check on mod7: Linearity
```{r}
logeq_volum <- log(eq_volum)
linear_price_c <- ggplot(Datset, aes(x=price_c, y=eq_volum)) + 
  coord_cartesian(xlim =c(0.83, .97), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)
linear_price_p <- ggplot(Datset, aes(x=price_p, y=eq_volum)) + 
  coord_cartesian(xlim =c(0.55, .8), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)
linear_disacv_c <- ggplot(Datset, aes(x=disacv_c, y=eq_volum)) + 
  coord_cartesian(xlim =c(5, 34), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)
linear_bonusacv <- ggplot(Datset, aes(x=bonusacv, y=eq_volum)) + 
  coord_cartesian(xlim =c(0, 70), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)
linear_tvgrp_c <- ggplot(Datset, aes(x=tvgrp_c, y=eq_volum)) + 
  coord_cartesian(xlim =c(0, 350), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)
linear_tvgrp_u <- ggplot(Datset, aes(x=tvgrp_u, y=eq_volum)) + 
  coord_cartesian(xlim =c(0, 400), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)
linear_itemstor <- ggplot(Datset, aes(x=itemstor, y=eq_volum)) + 
  coord_cartesian(xlim =c(8, 10), ylim = c(0, 15000000)) +
  geom_point()+
  geom_smooth(method=lm)

grid.arrange(linear_price_c, linear_price_p, linear_disacv_c, linear_bonusacv, linear_tvgrp_c, linear_tvgrp_u, linear_itemstor, nrow = 3)
```

As shown above, each of the predictor variables shows low linearity with the response variable.

Let's see the linearity after log transformation which we used for mod7.
```{r}
loglinear_price_c <- ggplot(Datset, aes(x=price_c, y=logeq_volum)) + 
  coord_cartesian(xlim =c(0.83, .97), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)
loglinear_price_p<- ggplot(Datset, aes(x=price_p, y=logeq_volum)) + 
  coord_cartesian(xlim =c(0.55, .8), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)
loglinear_disacv_c<- ggplot(Datset, aes(x=disacv_c, y=logeq_volum)) + 
  coord_cartesian(xlim =c(5, 34), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)
loglinear_bonusacv<- ggplot(Datset, aes(x=bonusacv, y=logeq_volum)) + 
  coord_cartesian(xlim =c(0, 70), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)
loglinear_tvgrp_c<- ggplot(Datset, aes(x=tvgrp_c, y=logeq_volum)) + 
  coord_cartesian(xlim =c(0, 350), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)
loglinear_tvgrp_u<- ggplot(Datset, aes(x=tvgrp_u, y=logeq_volum)) + 
  coord_cartesian(xlim =c(0, 400), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)
loglinear_itemstor<- ggplot(Datset, aes(x=itemstor, y=logeq_volum)) + 
  coord_cartesian(xlim =c(8, 10), ylim = c(12, 20)) +
  geom_point()+
  geom_smooth(method=lm)

grid.arrange(loglinear_price_c, loglinear_price_p, loglinear_disacv_c, loglinear_bonusacv, loglinear_tvgrp_c, loglinear_tvgrp_u, loglinear_itemstor, nrow = 3)
```

After a log transformation, the relationship is much more linear. This is a good indication that we have built a strong model.



### Assumption check on mod7: Do the residuals have equal variance?
```{r}
residuals7 <- residuals(mod7)
head(eq_volum)
head(predicted7)
head (residuals7)
residuals7

# plot residuals
plot(mod7,which=1)
```

As shown above, the model does display more of a heteroscedastic pattern, given that the residuals are not evenly distributed. Given that each of the largest residual values occurs at observations coinciding with the high spike sales around week 52, this sales spike is likely the cause of the residual outliers. We attempted to combat this effect by implementing a dummy “holiday” variable for sales during this week, but this approach fell short of producing a homoscedasticity of errors.

```{r}
bptest(mod7)
```

Breusch-Pagan is significant (<0.05), there is heteroscedasticity in our mod7 model


### Assumption check on mod7: Are the residuals normally distributed?
```{r}
resi_hist <- ggplot(Datset, aes(x=residuals7)) + geom_histogram()
resi_dens <- ggplot(Datset,aes(x=residuals7)) +
  geom_density()

grid.arrange(resi_hist, resi_dens, nrow = 1)

# Q-Q plot
QQplot <- plot(mod7,which=2)

```

As shown above, the residuals are distributed fairly normally, with exception to a couple of outliers. This is further evidence for a strong regression model. Here the brief departure in residual normality is attributed to the “holiday” spike in sales described previously.

We will now visualize in-sample and out-of-sample fit

```{r}
dim(Datset)
length(eq_volum)
length(eq_volum)*.75
insample=1:(length(eq_volum)*.75)
outsample=c(1:length(eq_volum))[-insample]
tail(insample)
head(outsample)

# Create Training and Test data

trainingData <- Datset[insample, ]  # model training data
testData  <- Datset[outsample, ]   # test data
```


#### Build the model on training data
```{r}
lmtrain <- lm(log(eq_volum) ~ price_c+price_p+disacv_c+bonusacv+holiday+tvgrp_c+tvgrp_u+itemstor, data=trainingData)  # build the model
Logtrainpred<-predict(lmtrain,trainingData)
trainpred=exp(Logtrainpred)
Logtestpred <- predict(lmtrain, testData)  # predict sales2
testpred=exp(Logtestpred)
```


#### We could look at actual v. predicted these ways
First, in-sample
```{r}
ggplot(trainingData, aes(observation)) + 
  geom_line(aes(y = eq_volum, colour = "eq_volum"),stat = "summary", fun = "mean") + 
  geom_line(aes(y = trainpred, colour = "trainpred"),stat = "summary", fun = "mean") 
```

Second, out of sample
```{r}
ggplot(testData, aes(observation)) + 
  geom_line(aes(y = eq_volum, colour = "eq_volum"),stat = "summary", fun = "mean") + 
  geom_line(aes(y = testpred, colour = "testpred"),stat = "summary", fun = "mean") 
```

## Part 5: What-if analysis. Managing marketing mix of Brand C using mod7

```{r}
summary(mod7)
```

Assume Brand C wants to predict their sales based on different marketing activities but first they want to know what their baseline sales are in the absence of any marketing activities and competitive activities. The intercept of mod7 will represent the baseline level of sales when all other variables are equal to zero. Brand C can expect to sell about exp(12.81) which is around 365857.8 units in the absence of any marketing activity and competitive activity. Let's say Brand C wants to predict what their sales will look like during holidays. This can be achieved by multiplying holiday’s coefficient with 1 to see the effect of holiday on sales. If they were to predict sales not during holidays, they can multiply holiday’s coefficient with 0 to see the effect of non-holidays on sales.

To understand the effect of pricing on sales of Brand C, we can create scenarios where for example, how does sales vary when price_c = 1.2 and price_p = 0.8 compared to when price_c = 1.15 and price_p = 0.75 assuming we get information on price reduction for Brand P. Using the model, we can understand how Brand C’s sales will change if our Competitor Brand P lowered their price by 0.05 and Brand C is thinking of lowering their price by 0.05 as well.

Let’s say Brand C wants to test the effect of increasing their TV advertisement viewing frequency by from an average of 500 GRPs to an average of 800 GRPs, we can feed the values in the model and calculate the sales increase:
```
exp(800*0.000538+12.8073911) - exp(500*0.000538+12.8073911) = 83642.48
```
This suggests that, keeping all other variables constant, an increase of Brand C’s TV GRPs from 500 to 800 will lead to an increase of 83642.48 unit sales. Taking consideration into costs of TV advertisement, Brand C will be able to use this predicted increase in unit sales to evaluate the cost effectiveness of a 300 GRPs increase.


**What-if analysis:**

The equation for the model can be written as:
```
log(sales) = 12.8073911 - 1.7618260*price_c + 0.4450893*price_p + 0.0302460*disacv_c + 0.0073333*bonusacv + 0.5240258*holiday1 + 0.0005380*tvgrp_c + 0.0003943*tvgrp_u + 0.3249773*itemstor
```
Brand C wants to know what their predicted sales will be when price_c = 0.9, price_p = 0.8, disacv_c = 15 and holiday = 0 (non-holiday) while keeping all other variables constant. We will have the following regression formula:
```
log(sales) = 12.8073911 - (1.7618260*0.9) + (0.4450893*0.8) + (0.0302460*15) + 0.00733330 + (0.5240258*0) + 0.000538 + 0.0003943 + 0.3249773
```
Based on the formula above, we will get a predicted sales of exp(12.36475) which is 234392 unit sales.
